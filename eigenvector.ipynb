{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "from NetworkStructure import Generator\n",
    "from NetworkStructure import Discriminator\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.sparse.linalg import eigsh\n",
    "from torch.autograd import Variable\n",
    "from torch.functional import Tensor\n",
    "from torchvision import transforms\n",
    "from torch import nn\n",
    "import torch\n",
    "import torchextractor as tx\n",
    "import torchvision\n",
    "%matplotlib inline\n",
    "import torch"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# parameters\n",
    "image_size = 28\n",
    "img_shape = 1,28,28\n",
    "latent_dim = 100\n",
    "image_channel = 1\n",
    "batch_size = 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "data_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.CenterCrop(28),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(0.5, 0.5),\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(\"./data\", train=True, download=True,transform=data_transform)\n",
    "# test_dataset = torchvision.datasets.MNIST(\"./data\", train=False,download=True,transform=data_transform)\n",
    "# min_test = torch.utils.data.Subset(test_dataset, torch.arange(1000))\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size= batch_size,\n",
    "    num_workers=2,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "# train_data = enumerate(train_dataloader)\n",
    "# a = next(train_data)\n",
    "# a[1][0].shape\n",
    "# enumerate = tuple(0,data) tuple(1,data) tuple(number,data)\n",
    "# iter = list[data]\n",
    "# data = list[data,label]\n",
    "train_data = iter(train_dataloader)\n",
    "# a = next(train_data)\n",
    "# a[0].shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Size([1, 1, 28, 28]), torch.Size([1, 100]))"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare noisy Data\n",
    "# get fist batch to train\n",
    "real_images, label = next(train_data)\n",
    "noisy = Variable(Tensor(np.random.normal(0, 1, (real_images.shape[0], latent_dim))))\n",
    "# we can't create noisy at first, because the last batch may be smaller than batch_size\n",
    "real_images.shape, noisy.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "Discriminator(\n  (model): Sequential(\n    (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n    (2): Dropout2d(p=0.25, inplace=False)\n    (3): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n    (5): Dropout2d(p=0.25, inplace=False)\n    (6): BatchNorm2d(32, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)\n    (7): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n    (8): LeakyReLU(negative_slope=0.2, inplace=True)\n    (9): Dropout2d(p=0.25, inplace=False)\n    (10): BatchNorm2d(64, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)\n    (11): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n    (12): LeakyReLU(negative_slope=0.2, inplace=True)\n    (13): Dropout2d(p=0.25, inplace=False)\n    (14): BatchNorm2d(128, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)\n  )\n  (adv_layer): Sequential(\n    (0): Linear(in_features=512, out_features=1, bias=True)\n    (1): Sigmoid()\n  )\n)"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH_G = \"./pre-train/checkpoint389G\"\n",
    "PATH_D = \"./pre-train/checkpoint389D\"\n",
    "checkpoint_G = torch.load(PATH_G)\n",
    "checkpoint_D = torch.load(PATH_D)\n",
    "model_G = Generator(image_size, latent_dim, image_channel)\n",
    "model_D = Discriminator(image_channel)\n",
    "model_G.load_state_dict(checkpoint_G['model_state_dict']) # print every layer name\n",
    "model_D.load_state_dict(checkpoint_D['model_state_dict'])\n",
    "model_G.eval()\n",
    "model_D.eval()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "# Function using to calculate Lapula matrix\n",
    "def Lapula(feature, number, nor=False):\n",
    "    \"\"\"\n",
    "    Calculate lapula metrix\n",
    "    :param feature: input the feature\n",
    "    :param number:  the number of feature\n",
    "    :param nor:  default is False, if true Lapula matrix will be normalize\n",
    "    :return:  lapura or sys lapula\n",
    "    \"\"\"\n",
    "    # 1. Calcute the Similarity matrix and Degree matrix\n",
    "    similarity_martix = np.empty((number,number),dtype=float)\n",
    "    d_matrix = np.zeros((number,number))\n",
    "    for i in range(feature.shape[0]):\n",
    "        x1 = feature[i]\n",
    "        for r  in range(i,feature.shape[0]):\n",
    "            if i == r:\n",
    "                similarity_martix[i][r] = 0\n",
    "            else:\n",
    "                x2 = feature[r]\n",
    "                weight = torch.nn.functional.cosine_similarity(x1,x2,0).detach().numpy()\n",
    "                # print(weight)\n",
    "                if weight >= 0.7:\n",
    "                    d_matrix[i][i] += 1\n",
    "                    similarity_martix[i][r]= 1-weight\n",
    "                    similarity_martix[r][i]= 1-weight\n",
    "                else:\n",
    "                    similarity_martix[i][r]= 0\n",
    "                    similarity_martix[r][i]= 0\n",
    "    lapula = d_matrix - similarity_martix\n",
    "    if nor==True:\n",
    "         t = np.sum(d_matrix, axis= 1)\n",
    "         V = np.diag(t**(-0.5))\n",
    "         where_are_inf = np.isinf(V)\n",
    "         V[where_are_inf] = 0\n",
    "         lapula = V@lapula@V\n",
    "\n",
    "    return lapula"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Size([1, 1, 28, 28]), torch.Size([1]))"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = next(train_data)\n",
    "data[0].shape, data[1].shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "(['',\n  'l1',\n  'l1.0',\n  'conv_blocks',\n  'conv_blocks.0',\n  'conv_blocks.1',\n  'conv_blocks.2',\n  'conv_blocks.3',\n  'conv_blocks.4',\n  'conv_blocks.5',\n  'conv_blocks.6',\n  'conv_blocks.7',\n  'conv_blocks.8',\n  'conv_blocks.9',\n  'conv_blocks.10'],\n ['',\n  'model',\n  'model.0',\n  'model.1',\n  'model.2',\n  'model.3',\n  'model.4',\n  'model.5',\n  'model.6',\n  'model.7',\n  'model.8',\n  'model.9',\n  'model.10',\n  'model.11',\n  'model.12',\n  'model.13',\n  'model.14',\n  'adv_layer',\n  'adv_layer.0',\n  'adv_layer.1'])"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show layer name in model\n",
    "G_name_list = tx.list_module_names(model_G)\n",
    "D_name_list = tx.list_module_names(model_D)\n",
    "G_name_list,D_name_list"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "(Extractor(\n   (model): Discriminator(\n     (model): Sequential(\n       (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n       (1): LeakyReLU(negative_slope=0.2, inplace=True)\n       (2): Dropout2d(p=0.25, inplace=False)\n       (3): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n       (4): LeakyReLU(negative_slope=0.2, inplace=True)\n       (5): Dropout2d(p=0.25, inplace=False)\n       (6): BatchNorm2d(32, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)\n       (7): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n       (8): LeakyReLU(negative_slope=0.2, inplace=True)\n       (9): Dropout2d(p=0.25, inplace=False)\n       (10): BatchNorm2d(64, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)\n       (11): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n       (12): LeakyReLU(negative_slope=0.2, inplace=True)\n       (13): Dropout2d(p=0.25, inplace=False)\n       (14): BatchNorm2d(128, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)\n     )\n     (adv_layer): Sequential(\n       (0): Linear(in_features=512, out_features=1, bias=True)\n       (1): Sigmoid()\n     )\n   )\n ),\n Extractor(\n   (model): Generator(\n     (l1): Sequential(\n       (0): Linear(in_features=100, out_features=6272, bias=True)\n     )\n     (conv_blocks): Sequential(\n       (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n       (1): Upsample(scale_factor=2.0, mode=nearest)\n       (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n       (3): BatchNorm2d(128, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)\n       (4): LeakyReLU(negative_slope=0.2, inplace=True)\n       (5): Upsample(scale_factor=2.0, mode=nearest)\n       (6): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n       (7): BatchNorm2d(64, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)\n       (8): LeakyReLU(negative_slope=0.2, inplace=True)\n       (9): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n       (10): Tanh()\n     )\n   )\n ))"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Specify the intermediate layer to be extracted\n",
    "layer_G = tx.Extractor(model_G, [\"conv_blocks.7\"])\n",
    "layer_D = tx.Extractor(model_D, [\"model.11\"])\n",
    "layer_D,layer_G"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Size([1, 64, 28, 28]),\n torch.Size([1, 128, 2, 2]),\n torch.Size([1, 128, 2, 2]))"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract model layer and get final result\n",
    "# get feature map in the model\n",
    "gen_img, p = layer_G(noisy,image_size,latent_dim)\n",
    "lable_real, q = layer_D(real_images)\n",
    "label_fake, r = layer_D(gen_img)\n",
    "x = real_images\n",
    "y = gen_img\n",
    "fp_G = p[\"conv_blocks.7\"] # feature map in generator 100 should be a parameter(100,64,28,28)\n",
    "fp_RD = q[\"model.11\"]   # real image feature map in discriminator(100,128,2,2)\n",
    "fp_FD = r[\"model.11\"]    # fake image feature map in discriminator\n",
    "# gen_img.shape, label_fake.shape, lable_real.shape\n",
    "fp_G.shape, fp_FD.shape, fp_RD.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Size([64, 28, 28]),\n tensor([[[-1.0761e-01, -1.2602e-01, -1.2646e-01,  ..., -6.2875e-02,\n           -5.4831e-02, -6.9094e-02],\n          [-1.6978e-01, -2.1704e-01, -1.7523e-01,  ..., -8.4729e-02,\n           -5.1808e-02, -6.5161e-02],\n          [-1.6812e-01, -2.1392e-01, -1.7756e-01,  ..., -9.4742e-02,\n           -4.3198e-02, -6.0152e-02],\n          ...,\n          [ 2.4883e-02,  2.3514e-01,  1.9436e-01,  ..., -2.9113e-01,\n           -3.1795e-01, -2.0354e-01],\n          [-4.5404e-02,  6.6265e-02,  6.5637e-02,  ..., -2.7911e-01,\n           -2.9602e-01, -1.9108e-01],\n          [-9.6430e-02, -7.2642e-02, -6.0554e-02,  ..., -1.9827e-01,\n           -2.1935e-01, -1.7033e-01]],\n \n         [[-1.1246e-01, -2.1246e-01, -1.9857e-01,  ...,  2.3866e-01,\n           -3.9592e-03, -2.7000e-02],\n          [-2.3597e-01, -1.9898e-01, -1.1842e-01,  ..., -1.3961e-02,\n           -7.3809e-02, -1.1486e-01],\n          [-2.6732e-01, -2.2642e-01, -7.5273e-02,  ...,  2.4760e-02,\n           -9.7462e-02, -1.5518e-01],\n          ...,\n          [ 5.1634e-01, -2.3405e-02, -3.5303e-02,  ..., -2.5484e-01,\n           -1.3806e-01,  9.7734e-01],\n          [ 3.8372e-01,  6.7853e-02,  5.9959e-02,  ..., -1.9060e-01,\n           -2.9496e-02,  1.2805e+00],\n          [ 1.5057e-01,  5.3173e-01,  3.3871e-01,  ..., -1.2815e-02,\n            2.9478e-01,  7.5548e-01]],\n \n         [[-6.3138e-02, -6.0079e-02, -3.0016e-02,  ...,  3.8229e-02,\n           -4.2935e-02, -3.2685e-02],\n          [-7.9840e-02, -5.8486e-02,  1.2056e-01,  ..., -3.0017e-02,\n           -9.8347e-02, -4.8054e-02],\n          [-7.5373e-02, -3.8319e-02,  1.5784e-01,  ..., -2.0372e-02,\n           -9.3028e-02, -4.9040e-02],\n          ...,\n          [ 7.7736e-02, -9.0994e-02, -7.2990e-02,  ..., -3.5491e-02,\n            2.2206e-01,  1.4565e+00],\n          [ 1.2373e-01, -6.0823e-02, -3.9679e-02,  ...,  7.7489e-02,\n            4.5511e-01,  1.4730e+00],\n          [ 2.7128e-01, -4.6636e-03,  4.5636e-02,  ...,  4.8577e-01,\n            7.0653e-01,  1.0532e+00]],\n \n         ...,\n \n         [[-1.9537e-01, -1.1880e-01, -1.7852e-03,  ...,  4.7615e-01,\n           -2.5788e-02, -3.3135e-02],\n          [-1.6977e-01, -4.2480e-02,  2.7500e-01,  ...,  5.6999e-01,\n           -7.3951e-02, -4.3235e-02],\n          [-1.6360e-01, -4.9656e-02,  8.1297e-02,  ...,  5.4527e-01,\n           -7.6848e-02, -4.1471e-02],\n          ...,\n          [-9.5584e-03, -7.6983e-03, -2.0002e-02,  ..., -1.0107e-02,\n            6.9464e-01,  1.8376e+00],\n          [-2.6943e-02,  1.9337e-01,  2.4907e-01,  ...,  4.0206e-01,\n            1.0233e+00,  1.7208e+00],\n          [ 1.0396e-01,  4.4369e-01,  3.8127e-01,  ...,  8.3710e-01,\n            1.1850e+00,  1.2575e+00]],\n \n         [[ 4.4370e-01,  5.2474e-01,  6.0634e-01,  ...,  9.6704e-01,\n            1.0978e+00,  8.0974e-01],\n          [ 6.8573e-01,  1.0172e+00,  1.1008e+00,  ...,  1.2201e+00,\n            1.3294e+00,  9.9105e-01],\n          [ 5.8917e-01,  9.5881e-01,  1.2263e+00,  ...,  8.9908e-01,\n            1.0803e+00,  8.9314e-01],\n          ...,\n          [-2.7745e-02, -3.9645e-02, -5.0200e-02,  ...,  1.6030e+00,\n            1.5196e+00,  9.7726e-01],\n          [-8.8412e-03, -4.1865e-02, -3.4144e-02,  ...,  1.4935e+00,\n            1.3402e+00,  9.5383e-01],\n          [ 4.0937e-01,  2.2813e-01,  1.9660e-01,  ...,  1.5656e+00,\n            1.5987e+00,  1.2082e+00]],\n \n         [[ 9.4369e-02,  3.0262e-01,  1.0926e-01,  ...,  2.7324e-01,\n            6.5073e-02,  9.7611e-02],\n          [ 5.7674e-01,  5.1066e-01,  1.9125e-01,  ...,  4.2613e-01,\n            1.7720e-01,  3.3853e-01],\n          [ 8.2157e-01,  6.1698e-01,  2.4336e-01,  ...,  3.6381e-01,\n            2.9028e-01,  3.2939e-01],\n          ...,\n          [-9.4352e-02, -8.7435e-02, -6.2091e-02,  ...,  1.0230e+00,\n            1.0266e+00,  9.5801e-01],\n          [-3.5551e-02, -2.8804e-03, -4.1158e-02,  ...,  8.7585e-01,\n            7.7703e-01,  7.3631e-01],\n          [ 1.5439e-01,  1.9549e-01, -4.6322e-03,  ...,  9.4441e-01,\n            7.2768e-01,  2.9316e-01]]], grad_fn=<SqueezeBackward0>))"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_map = fp_G.squeeze()\n",
    "feature_map.shape, feature_map"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 9.4369e-02,  3.0262e-01,  1.0926e-01, -3.8229e-02, -1.9867e-02,\n         -3.7250e-03, -4.0667e-02, -1.8109e-02, -7.1330e-03,  1.0705e-01,\n          3.2419e-01,  4.7300e-01,  3.7063e-01,  6.3410e-01,  5.7043e-01,\n          5.8309e-01,  7.5920e-01,  7.9114e-01,  9.2384e-01,  9.3998e-01,\n          1.0085e+00,  7.2787e-01,  7.1450e-01,  7.4326e-01,  5.6788e-01,\n          2.7324e-01,  6.5073e-02,  9.7611e-02],\n        [ 5.7674e-01,  5.1066e-01,  1.9125e-01, -2.5366e-02, -4.2182e-02,\n         -9.2979e-02, -1.4857e-01, -1.5847e-01, -1.4121e-01, -4.2022e-02,\n          3.9370e-01,  1.0306e+00,  9.9937e-01,  1.0604e+00,  1.1230e+00,\n          9.4240e-01,  1.0961e+00,  1.3472e+00,  1.4930e+00,  1.3163e+00,\n          1.1867e+00,  7.0457e-01,  4.6669e-01,  5.9703e-01,  3.9487e-01,\n          4.2613e-01,  1.7720e-01,  3.3853e-01],\n        [ 8.2157e-01,  6.1698e-01,  2.4336e-01, -4.6614e-02, -6.6718e-02,\n         -1.3302e-01, -1.7091e-01, -1.6234e-01, -1.2691e-01,  1.0342e-01,\n          5.8074e-01,  1.1553e+00,  9.7225e-01,  8.4062e-01,  9.6888e-01,\n          1.0049e+00,  1.2573e+00,  1.7430e+00,  2.0054e+00,  2.0192e+00,\n          1.8685e+00,  1.1972e+00,  6.3340e-01,  4.6317e-01,  3.3805e-01,\n          3.6381e-01,  2.9028e-01,  3.2939e-01],\n        [ 5.9857e-01,  3.7800e-01,  2.2197e-01, -5.8331e-02, -6.5693e-02,\n         -1.2640e-01, -1.9050e-01, -8.7845e-02, -1.1256e-02,  1.0491e+00,\n          1.5263e+00,  1.5331e+00,  1.1786e+00,  5.0634e-01,  3.7063e-01,\n         -7.2834e-03,  2.1443e-01,  2.7128e-01,  3.3416e-01,  5.8009e-01,\n          5.6549e-01,  3.5880e-01,  1.1939e-01,  1.1786e-01,  1.5369e-01,\n          9.0325e-02,  2.3832e-02,  5.9517e-01],\n        [ 4.7375e-01,  3.0743e-01,  2.4288e-01, -3.5826e-02, -2.7464e-02,\n         -8.5300e-02, -1.2435e-01, -5.4251e-03,  2.8811e-01,  1.3009e+00,\n          1.3291e+00,  8.7601e-01,  4.4163e-01, -4.4216e-02, -5.9255e-02,\n         -1.4354e-01, -1.1025e-01, -1.1416e-01, -9.7306e-02, -3.6164e-02,\n         -1.2025e-02, -1.9698e-02, -4.0514e-02, -9.7652e-03, -3.2610e-03,\n         -9.3523e-03,  3.2833e-02,  5.8483e-01],\n        [ 1.1114e-01,  1.9761e-01,  3.0756e-01,  9.1592e-02,  1.3581e-01,\n         -4.4762e-02, -7.0677e-02,  6.7646e-01,  1.0955e+00,  1.6007e+00,\n          1.1638e+00, -4.7626e-02, -1.8482e-01, -2.4619e-01, -2.3259e-01,\n         -1.7625e-01, -1.2953e-01, -1.0938e-01, -6.3073e-02, -6.6225e-02,\n         -3.3726e-02, -6.4236e-02, -6.9567e-02, -2.0501e-02, -7.7147e-03,\n         -2.1029e-02,  7.2517e-02,  2.5105e-01],\n        [-1.5965e-02,  1.3261e-01,  2.8786e-01,  1.4797e-01,  2.2168e-01,\n         -5.5257e-03, -1.2073e-02,  1.0816e+00,  1.2944e+00,  1.3335e+00,\n          8.2646e-01, -1.0196e-01, -2.2630e-01, -2.9131e-01, -2.8877e-01,\n         -2.3749e-01, -1.8134e-01, -1.5863e-01, -1.0679e-01, -1.2996e-01,\n         -8.8610e-02, -1.0213e-01, -8.2425e-02, -2.4483e-02, -3.5078e-02,\n         -1.5674e-02,  2.5526e-02,  2.2505e-01],\n        [ 6.1537e-02,  2.5109e-01,  3.8314e-01,  1.6660e-01,  1.6441e-01,\n          9.9314e-02,  2.6345e-01,  1.0526e+00,  1.0646e+00,  4.2011e-01,\n         -1.1083e-02, -1.6684e-01, -2.5044e-01, -2.5860e-01, -2.9044e-01,\n         -2.0197e-01, -1.6898e-01, -1.0969e-01, -2.8141e-02, -6.6109e-02,\n         -2.3726e-02, -3.9778e-02, -1.5366e-02,  8.3189e-02, -1.6850e-02,\n         -2.4619e-03, -7.4883e-03, -6.5062e-03],\n        [-5.9236e-02, -5.7082e-02, -1.3312e-02, -1.6867e-02,  5.1454e-03,\n         -7.9627e-03, -6.4615e-03,  6.4175e-01,  6.3858e-01, -4.0504e-03,\n         -5.5531e-02, -2.0305e-01, -2.7402e-01, -2.7391e-01, -2.7085e-01,\n         -1.2832e-01, -8.6479e-02, -7.1032e-03,  2.7888e-01, -1.3383e-02,\n          2.5734e-02, -1.3848e-02,  6.0297e-02,  3.1090e-01,  9.3217e-02,\n          1.5801e-01,  9.0530e-02,  1.0715e-01],\n        [-1.2117e-02, -1.7225e-01, -1.9002e-01, -2.1369e-01, -1.6459e-01,\n         -5.1405e-02, -1.4458e-02,  6.0382e-01,  4.6638e-01, -1.0204e-01,\n         -1.6051e-01, -2.6467e-01, -2.9612e-01, -1.7189e-01, -1.2853e-01,\n          4.1254e-02,  3.0542e-01,  2.3875e-01,  3.9329e-01, -2.7668e-02,\n         -3.1234e-02,  8.3874e-03, -3.3544e-04,  1.6131e-01, -3.2052e-02,\n          5.3403e-02,  6.7067e-02,  8.5689e-02],\n        [-2.7764e-02, -2.4623e-01, -2.3748e-01, -2.3933e-01, -1.8383e-01,\n         -4.1776e-02, -5.1319e-02,  1.9402e-01,  1.0440e-01, -1.5670e-01,\n         -1.7285e-01, -2.2960e-01, -2.0912e-01, -4.9762e-02, -1.8159e-02,\n          3.5287e-01,  3.8694e-01,  1.3104e-01,  2.4003e-01, -2.1257e-02,\n         -1.6576e-02,  4.2186e-01,  4.5279e-01,  6.3868e-01,  4.0491e-01,\n          3.3637e-01,  1.6888e-01, -7.6928e-03],\n        [-1.2186e-03, -2.1176e-01, -2.2458e-01, -1.5681e-01, -9.5089e-02,\n          4.0274e-01,  4.2446e-01,  5.1048e-01,  4.3057e-01, -6.0735e-02,\n         -1.0211e-01, -1.2212e-01, -1.0543e-01,  5.1047e-02,  4.2872e-02,\n         -5.8025e-02, -6.9200e-02, -1.4759e-01, -1.3112e-01, -1.3349e-01,\n         -1.1764e-01,  1.9162e-01,  2.4390e-01,  6.9018e-01,  6.4676e-01,\n          6.1545e-01,  4.7362e-01,  1.2715e-01],\n        [ 2.0262e-01, -1.3579e-01, -1.3984e-01, -6.8273e-02, -2.2783e-02,\n          6.7446e-01,  6.4391e-01,  5.5021e-01,  5.2851e-01, -3.0737e-02,\n         -7.8208e-02, -7.2149e-02, -5.0366e-02,  4.9993e-02, -2.3784e-02,\n         -1.6084e-01, -1.5799e-01, -1.8656e-01, -1.5534e-01, -1.0574e-01,\n         -8.2352e-02,  3.6846e-01,  5.1339e-01,  1.0128e+00,  8.8611e-01,\n          7.0410e-01,  4.6298e-01,  7.7260e-02],\n        [-7.4620e-02, -1.5775e-01, -1.4648e-01, -1.9690e-02,  1.7129e-01,\n          5.8407e-01,  6.3917e-01,  4.9353e-01,  6.5443e-01,  5.4082e-01,\n          3.5411e-01,  4.2889e-01,  4.6055e-01, -1.1383e-02, -1.1147e-01,\n         -2.1559e-01, -2.1699e-01, -8.7590e-02, -7.9214e-03,  1.4883e-01,\n          1.3894e-01,  4.8863e-01,  6.1790e-01,  9.4287e-01,  8.2964e-01,\n          3.6430e-01,  1.2829e-01, -1.3386e-02],\n        [-1.4827e-02, -6.6070e-02, -8.0807e-02,  2.9375e-02,  1.7647e-01,\n          5.6635e-01,  8.7883e-01,  8.4928e-01,  1.0216e+00,  1.1157e+00,\n          7.7615e-01,  7.2558e-01,  6.6528e-01, -3.5623e-02, -1.2124e-01,\n         -2.1566e-01, -2.1208e-01, -1.1716e-01, -3.0577e-02, -8.4005e-03,\n          6.0860e-02,  4.0474e-01,  3.4818e-01,  7.0857e-01,  5.9528e-01,\n          3.1073e-01,  1.5083e-01,  9.8692e-02],\n        [ 8.8436e-02,  4.7472e-02,  3.8251e-02,  2.3076e-01,  2.5929e-01,\n          7.7119e-01,  1.1241e+00,  8.3444e-01,  7.8178e-01,  6.3094e-01,\n          2.0693e-01,  1.2056e-01,  1.9656e-01, -1.0438e-01, -1.0908e-01,\n         -1.3569e-01, -1.7962e-01, -7.4489e-02, -5.0071e-02, -7.7553e-03,\n          1.6903e-01,  5.4948e-01,  5.8514e-01,  7.8129e-01,  8.0101e-01,\n          5.1523e-01,  4.8529e-01,  3.4077e-01],\n        [-3.5218e-02, -2.3669e-03,  1.2087e-01,  1.3411e-01,  1.4371e-01,\n          6.4752e-01,  9.4468e-01,  5.8761e-01,  4.1998e-01,  2.9071e-01,\n         -1.7477e-02, -2.3352e-02,  2.8113e-02, -8.8744e-02, -7.5256e-02,\n         -8.1256e-02, -1.2678e-01, -5.6044e-02, -3.1006e-02, -9.1026e-03,\n          5.4790e-02,  3.5776e-01,  3.5168e-01,  4.2693e-01,  5.5287e-01,\n          4.0088e-01,  3.4616e-01,  2.6765e-01],\n        [-2.0540e-02,  2.0844e-02,  1.3955e-01,  1.2379e-01,  1.3787e-01,\n          8.0163e-01,  1.0430e+00,  4.6256e-01,  2.8748e-01, -2.5187e-02,\n         -1.2526e-01, -1.5582e-01, -1.0171e-01, -1.5108e-02,  3.7342e-01,\n          3.0607e-01,  1.4053e-01,  2.4239e-01,  1.5519e-01,  2.3215e-01,\n          3.0757e-01,  8.4915e-01,  1.0054e+00,  1.1817e+00,  1.0996e+00,\n          6.1548e-01,  4.8449e-01,  4.0311e-01],\n        [ 8.6809e-02, -1.1668e-02, -1.5025e-02,  1.0214e-01,  6.6699e-02,\n          4.4203e-01,  4.1167e-01, -5.9085e-02, -6.9446e-02, -1.4512e-01,\n         -1.7185e-01, -1.6907e-01, -1.2432e-01,  3.0329e-01,  6.5920e-01,\n          5.5194e-01,  4.0976e-01,  5.3820e-01,  4.8953e-01,  6.4770e-01,\n          6.1776e-01,  1.0120e+00,  1.0855e+00,  1.2348e+00,  1.2251e+00,\n          5.2492e-01,  4.7548e-01,  4.8759e-01],\n        [ 4.2298e-01,  2.2857e-01,  1.9123e-01,  3.5839e-01,  3.5191e-01,\n          3.1819e-01,  1.2231e-02, -1.7483e-01, -1.9083e-01, -2.2566e-01,\n         -2.2205e-01, -1.9441e-01, -1.8008e-01, -1.0610e-01, -8.7499e-02,\n         -1.3216e-01, -8.6736e-02,  1.3584e-01,  3.9402e-01,  1.0803e+00,\n          1.0937e+00,  1.3653e+00,  1.2577e+00,  1.1381e+00,  9.8338e-01,\n          3.2703e-01,  3.4116e-01,  2.6787e-01],\n        [ 5.2199e-01,  4.0551e-01,  3.7991e-01,  4.5447e-01,  3.1025e-01,\n          1.8178e-01, -6.8913e-03, -1.8660e-01, -1.8802e-01, -2.1558e-01,\n         -2.1322e-01, -1.9200e-01, -2.0565e-01, -1.8243e-01, -1.6664e-01,\n         -1.6868e-01, -1.0521e-01,  2.0676e-01,  5.2406e-01,  1.2523e+00,\n          1.2826e+00,  1.5988e+00,  1.3831e+00,  1.0611e+00,  8.0878e-01,\n          9.3794e-02,  1.1066e-01,  1.1694e-01],\n        [ 2.3475e-01,  3.7149e-01,  7.0919e-01,  8.2523e-01,  6.0511e-01,\n          1.7620e-01, -3.6293e-02, -1.3447e-01, -1.2524e-01, -8.3443e-02,\n         -6.4150e-02, -5.0056e-02, -7.9041e-02, -1.4210e-01, -1.5491e-01,\n         -1.7952e-01, -1.2667e-01,  9.7169e-02,  4.7028e-01,  1.1771e+00,\n          1.2041e+00,  1.5751e+00,  1.2339e+00,  6.4831e-01,  6.5638e-01,\n         -1.5620e-02,  4.0106e-02,  4.2062e-01],\n        [-3.9263e-03,  2.3482e-01,  6.7610e-01,  7.8381e-01,  7.1434e-01,\n          3.2370e-01, -1.0662e-03, -8.0868e-02, -9.6661e-02, -5.1190e-02,\n         -6.7840e-02, -9.4557e-02, -1.2376e-01, -1.9967e-01, -1.8282e-01,\n         -1.9306e-01, -1.1568e-01,  3.0816e-01,  4.8192e-01,  1.1761e+00,\n          1.2262e+00,  1.3114e+00,  8.1073e-01,  1.4629e-01,  1.8172e-01,\n         -2.3800e-02,  3.1456e-01,  7.6420e-01],\n        [-4.1029e-02, -4.8236e-02,  2.9697e-02,  7.1339e-02,  2.4203e-01,\n          2.1329e-01, -6.8534e-03, -6.7790e-03, -1.5842e-02,  1.9389e-01,\n          2.8654e-03, -7.5006e-02, -1.1695e-01, -1.6355e-01, -1.4173e-01,\n         -9.1890e-02, -2.3042e-02,  5.6745e-01,  6.9679e-01,  1.1559e+00,\n          1.2007e+00,  8.7491e-01,  4.3368e-01, -7.4245e-03,  1.5999e-01,\n          2.8485e-01,  6.9131e-01,  1.3021e+00],\n        [-9.3482e-02, -1.2435e-01, -6.4700e-02,  2.5075e-02,  3.3657e-01,\n          3.8435e-01,  1.5590e-01,  1.3483e-01,  5.4790e-02,  1.1888e-01,\n         -4.0412e-02, -1.1259e-01, -1.2585e-01, -1.5352e-01, -1.1956e-01,\n         -6.4743e-02, -3.1077e-02,  4.1894e-01,  7.0855e-01,  1.0844e+00,\n          1.0248e+00,  7.1164e-01,  5.0169e-01,  2.4235e-01,  5.6870e-01,\n          7.5171e-01,  9.8198e-01,  1.3705e+00],\n        [-9.4352e-02, -8.7435e-02, -6.2091e-02,  1.1477e-01,  3.9660e-01,\n          4.2031e-01,  2.0493e-01, -1.8036e-02, -2.5903e-02, -9.6894e-03,\n         -4.1840e-02, -7.1186e-02, -7.6945e-02, -8.4130e-02, -5.5183e-02,\n          2.1825e-02,  3.6819e-02,  4.3156e-01,  7.5706e-01,  9.4358e-01,\n          9.1597e-01,  5.2805e-01,  4.4171e-01,  5.6155e-01,  7.8397e-01,\n          1.0230e+00,  1.0266e+00,  9.5801e-01],\n        [-3.5551e-02, -2.8804e-03, -4.1158e-02, -3.6348e-02,  1.0523e-03,\n          2.1890e-01,  2.5269e-01,  1.6854e-01,  2.4621e-01,  3.6674e-01,\n          2.4876e-01,  1.2947e-01,  1.1203e-01,  1.5206e-01,  2.3667e-01,\n          4.7652e-01,  4.1580e-01,  6.7119e-01,  8.7807e-01,  9.6900e-01,\n          1.0574e+00,  7.0552e-01,  6.1410e-01,  6.5934e-01,  7.9622e-01,\n          8.7585e-01,  7.7703e-01,  7.3631e-01],\n        [ 1.5439e-01,  1.9549e-01, -4.6322e-03, -1.6905e-02,  3.2744e-02,\n          4.7079e-01,  5.1027e-01,  4.5108e-01,  5.2651e-01,  5.1962e-01,\n          4.4697e-01,  4.0343e-01,  5.0840e-01,  7.5404e-01,  9.6876e-01,\n          1.0852e+00,  1.0957e+00,  1.0749e+00,  1.0225e+00,  1.0191e+00,\n          1.0444e+00,  9.3583e-01,  9.4487e-01,  1.0348e+00,  1.0876e+00,\n          9.4441e-01,  7.2768e-01,  2.9316e-01]], grad_fn=<SliceBackward0>)"
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pixel_matrix = np.empty((28*28,64),dtype=float)\n",
    "img = feature_map[63,:,:]\n",
    "img"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "for i in range(feature_map.shape[0]):\n",
    "    for t in range(28):\n",
    "        for v in range(28):\n",
    "            pixel_matrix[t+v,i] = feature_map[i,t,v]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[-1.07614651e-01, -1.12462662e-01, -6.31382838e-02, ...,\n        -1.95367739e-01,  4.43696201e-01,  9.43686143e-02],\n       [-1.69781134e-01, -2.35971570e-01, -7.98403397e-02, ...,\n        -1.69773415e-01,  6.85731173e-01,  5.76738656e-01],\n       [-1.68121025e-01, -2.67323315e-01, -7.53728077e-02, ...,\n        -1.63595617e-01,  5.89173198e-01,  8.21569264e-01],\n       ...,\n       [-3.98023230e-08, -2.91499552e-08, -5.50414709e-07, ...,\n         4.03958266e-07,  9.46340111e-10, -2.50409958e-09],\n       [-1.60540565e-07, -8.11832259e-13, -4.15015174e-14, ...,\n         3.99140233e-04,  6.63595640e-07,  6.31018448e-03],\n       [ 8.87499189e-03,  2.63807887e-04,  2.55044885e-02, ...,\n         4.89492433e-07, -3.16253477e-13, -1.11757166e-15]])"
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pixel_matrix"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_836209/1354441053.py:32: RuntimeWarning: divide by zero encountered in power\n",
      "  V = np.diag(t**(-0.5))\n"
     ]
    }
   ],
   "source": [
    "ten_Pixel = torch.from_numpy(pixel_matrix)\n",
    "lapula = Lapula(ten_Pixel,28*28,True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "data": {
      "text/plain": "(784, 784)"
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lapula.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "# Function using to calculate eigenvalues and eigenvector\n",
    "def eigvalueAeigvector(matrix):\n",
    "\n",
    "    lapula_ten = torch.from_numpy(matrix)\n",
    "    eigenvalues, eigenvectors = torch.eig(lapula_ten, eigenvectors=True)\n",
    "    return [eigenvalues,eigenvectors]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Size([784, 2]), torch.Size([784, 784]))"
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eigenvalue, eigenvector = eigvalueAeigvector(lapula)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Size([784, 2]), torch.Size([784, 784]))"
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eigenvalue.shape,eigenvector.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [],
   "source": [
    "def sort(eigenvalues,eigenvectors):\n",
    "    sort_value = torch.empty((eigenvalues.shape[0],2),dtype=torch.float64)\n",
    "    sort_vector = torch.empty((eigenvectors.shape[0],eigenvectors.shape[1]),dtype=torch.float64)\n",
    "    # print(sort_vector.shape)\n",
    "    r = np.argsort(eigenvalues[:,0])\n",
    "    # print(r)\n",
    "    i = 0\n",
    "    for x in iter(r):\n",
    "        # print(eigenvalues[x,0:2])\n",
    "        sort_value[i,:]= eigenvalues[x,:]\n",
    "        sort_vector[:,i] = eigenvectors[:,x]\n",
    "        # sort_value[i,1]= eigenvalues[x,1]\n",
    "        i += 1\n",
    "    return sort_value, sort_vector"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [
    "s_eigenvalue, s_eigenvector = sort(eigenvalue,eigenvector)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Size([784, 2]), torch.Size([784, 784]))"
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_eigenvalue.shape, s_eigenvector.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([784, 1, 28, 28])"
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = s_eigenvector.reshape(-1,1,28,28)\n",
    "img.data.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([28, 28])"
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i_s = img.reshape(28,28)\n",
    "i_s.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "outputs": [
    {
     "data": {
      "text/plain": "<matplotlib.image.AxesImage at 0x7f2d0dcf72e0>"
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(i_s.detach().numpy().reshape(28,28))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "outputs": [],
   "source": [
    "from torchvision.utils import save_image\n",
    "\n",
    "save_image(img.data[:783], \"/home/athena/Pictures/GAN1/visual/real image/real5656.png\" ,nrow=27,normalize=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "outputs": [
    {
     "data": {
      "text/plain": "<matplotlib.image.AxesImage at 0x7f2d0dce4760>"
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "c = y.squeeze()\n",
    "plt.imshow(c.detach().numpy())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}